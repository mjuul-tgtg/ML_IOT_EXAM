{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References: \n",
    "https://www.kaggle.com/code/niyamatalmass/texts-summarizing-with-the-help-of-spacy/notebook\n",
    "https://spacy.io/usage/spacy-101\n",
    "https://www.analyticsvidhya.com/blog/2020/03/spacy-tutorial-learn-natural-language-processing/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from string import punctuation\n",
    "from heapq import nlargest\n",
    "import pymysql\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = pymysql.connect(host='localhost', user='admin',\n",
    "                            passwd=\"password\", db='joe_rogan')\n",
    "cur = conn.cursor()\n",
    "cur.execute(\n",
    "    \"SElECT id, episode_id, text FROM `transcript_time_split` \"\n",
    "    \"WHERE episode_id=269 \"\n",
    "    \"ORDER BY id ASC \"\n",
    "    \"LIMIT 10000\")\n",
    "data_tuple = cur.fetchall()\n",
    "cur.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for i in data_tuple:\n",
    "    data.append(list(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(data)):\n",
    "    data[i][2] = \" \".join(data[i][2].split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "doc = nlp(data[7][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "By --> adposition (ADP)\n",
      "the --> determiner (DET)\n",
      "way --> noun (NOUN)\n",
      ". --> punctuation (PUNCT)\n",
      "There --> pronoun (PRON)\n",
      "is --> verb (VERB)\n",
      "a --> determiner (DET)\n",
      "system --> noun (NOUN)\n",
      "like --> adposition (ADP)\n",
      "that --> pronoun (PRON)\n",
      "in --> adposition (ADP)\n",
      "the --> determiner (DET)\n",
      "brain --> noun (NOUN)\n",
      ". --> punctuation (PUNCT)\n"
     ]
    }
   ],
   "source": [
    "for token in range(14):\n",
    "    print(doc[token].text + \" --> \" + spacy.explain(doc[token].pos_) + \" (\" + doc[token].pos_ + \")\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "By --> None (ROOT)\n",
      "the --> determiner (det)\n",
      "way --> object of preposition (pobj)\n",
      ". --> punctuation (punct)\n",
      "There --> expletive (expl)\n",
      "is --> None (ROOT)\n",
      "a --> determiner (det)\n",
      "system --> attribute (attr)\n",
      "like --> prepositional modifier (prep)\n",
      "that --> object of preposition (pobj)\n",
      "in --> prepositional modifier (prep)\n",
      "the --> determiner (det)\n",
      "brain --> object of preposition (pobj)\n",
      ". --> punctuation (punct)\n"
     ]
    }
   ],
   "source": [
    "for token in range(14):\n",
    "    print(doc[token].text + \" --> \" + str(spacy.explain(doc[token].dep_)) + \" (\" + doc[token].dep_ + \")\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aeropostale\n",
      "    Aeropostale (ORG PROPN compound)\n",
      "last night\n",
      "    last (TIME ADJ amod)\n",
      "    night (TIME NOUN npadvmod)\n",
      "12 hours\n",
      "    12 (TIME NUM nummod)\n",
      "    hours (TIME NOUN pobj)\n",
      "no 16 hours\n",
      "    no (TIME DET det)\n",
      "    16 (TIME NUM nummod)\n",
      "    hours (TIME NOUN npadvmod)\n"
     ]
    }
   ],
   "source": [
    "for ent in doc.ents[:4]:\n",
    "    print(ent.text)\n",
    "    for word in ent:\n",
    "        print(\"    \" + word.text + \" (\" + ent.label_ + \" \" + word.pos_ + \" \" + word.dep_ + \")\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize(text, per, nounweight, verbweight, personweight, orgweight):\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    doc= nlp(text)\n",
    "    tokens=[token.text for token in doc]\n",
    "    word_frequencies={}\n",
    "    for word in doc:\n",
    "        if word.text.lower() not in list(STOP_WORDS):\n",
    "            if word.text.lower() not in punctuation:\n",
    "                if (word.pos_ == \"NOUN\" and word.dep_ == \"dobj\"):\n",
    "                    if word.text not in word_frequencies.keys():\n",
    "                        word_frequencies[word.text] = nounweight\n",
    "                    else:\n",
    "                        word_frequencies[word.text] += nounweight\n",
    "                elif (word.pos_ == \"VERB\" and word.dep_ in [\"ROOT\", \"advcl\"]):\n",
    "                    if word.text not in word_frequencies.keys():\n",
    "                        word_frequencies[word.text] = verbweight\n",
    "                    else:\n",
    "                        word_frequencies[word.text] += verbweight\n",
    "    for ent in doc.ents:\n",
    "        for word in ent:\n",
    "            if (ent.label_ == \"PERSON\") and (word.pos_ == \"PROPN\"):\n",
    "                if ent.text not in word_frequencies.keys():\n",
    "                    word_frequencies[ent.text] = personweight\n",
    "            elif (ent.label_ == \"ORG\") and (word.pos_ == \"PROPN\"):\n",
    "                if ent.text not in word_frequencies.keys():\n",
    "                    word_frequencies[ent.text] = orgweight\n",
    "    max_frequency=max(word_frequencies.values())\n",
    "    for word in word_frequencies.keys():\n",
    "        word_frequencies[word]=word_frequencies[word]/max_frequency\n",
    "    sentence_tokens= [sent for sent in doc.sents]\n",
    "    sentence_scores = {}\n",
    "    for sent in sentence_tokens:\n",
    "        for word in sent:\n",
    "            if word.text.lower() in word_frequencies.keys():\n",
    "                if sent not in sentence_scores.keys():                            \n",
    "                    sentence_scores[sent]=word_frequencies[word.text.lower()]\n",
    "                else:\n",
    "                    sentence_scores[sent]+=word_frequencies[word.text.lower()]\n",
    "    select_length=int(len(sentence_tokens)*per)\n",
    "    summary=nlargest(select_length, sentence_scores,key=sentence_scores.get)\n",
    "    final_summary=[word.text for word in summary]\n",
    "    summary=''.join(final_summary)\n",
    "    #pp.pprint(sorted(word_frequencies.items(), key=lambda kv: kv[1], reverse=True)[:10])\n",
    "    #pp.pprint(f\"sentence_scores{sorted(sentence_scores.items(), key=lambda kv: kv[1], reverse=True)}\")\n",
    "    #pp.pprint(f\"sentence_tokens{STOP_WORDS}\")\n",
    "    return summary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nounweight = 1\n",
    "verbweight = 1\n",
    "personweight = 1\n",
    "orgweight = 1\n",
    "\n",
    "output_summaries = []\n",
    "\n",
    "for i in data:\n",
    "    tempEpisodeID = i[1]\n",
    "    tempID = i[0]\n",
    "    #print(tempID)\n",
    "    tempOutput = summarize(i[2], 0.05, nounweight, verbweight, personweight, orgweight)\n",
    "    output_summaries.append([tempEpisodeID, tempID, tempOutput])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open ('spacy_summaries.txt','a') as f:\n",
    "    f.write(f\"\"\"\n",
    "\n",
    "    SPACY SUMMARIES\n",
    "    Tuned to:\n",
    "        nounweight = {nounweight}\n",
    "        verbweight = {verbweight}\n",
    "        personweight = {personweight}\n",
    "        orgweight = {orgweight}\n",
    "    \"\"\"\n",
    "    )\n",
    "\n",
    "    for i in range(len(output_summaries)):\n",
    "        f.write(f\"\"\"\n",
    "        EpisodeID: {output_summaries[i][0]}\n",
    "        10 minute ID: {output_summaries[i][1]}\n",
    "        Summary: \n",
    "        {output_summaries[i][2]}\n",
    "        \"\"\"\n",
    "        )\n",
    "        f.write(f\"\"\"\n",
    "        Length of original: {len(data[i][2])}, Length of summary: {len(output_summaries[i][2])}\n",
    "        \n",
    "        \"\"\"\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import spacy\\nfrom spacy import displacy\\n\\nnlp = spacy.load(\"en_core_web_sm\")\\ndoc = nlp(\"AI & ML is a fantastic course, and we love it\")\\ndisplacy.serve(doc, style=\"dep\")'"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"AI & ML is a fantastic course, and we love it\")\n",
    "displacy.serve(doc, style=\"dep\")\"\"\""
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
